{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc267d69",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac077cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colored output\n",
    "GREEN = \"\\033[92m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"  # White"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68056a95-bfb5-4df3-9507-61a813b24456",
   "metadata": {},
   "source": [
    "# La régression linéaire avec TensorFlow\n",
    "\n",
    "Bienvenue dans ce notebook sur la régression linéaire avec TensorFlow !\n",
    "\n",
    "## Comment Utiliser ce Tutoriel\n",
    "\n",
    "- **Pratiquez en Même Temps** : Essayez de reproduire le code sur votre propre machine pendant que vous lisez.\n",
    "- **Expérimentez** : N'hésitez pas à modifier le code et à observer comment cela affecte les résultats.\n",
    "- **Posez des Questions** : Si quelque chose n'est pas clair, prenez le temps de faire des recherches supplémentaires ou de demander de l'aide.\n",
    "\n",
    "---\n",
    "\n",
    "Prêt à plonger dans le monde passionnant de la régression linéaire avec TensorFlow ? Commençons notre voyage d'apprentissage ensemble !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708c62f-b972-4794-a0e7-84e20703ee05",
   "metadata": {},
   "source": [
    "# Un Bref Rappel Théorique de la Régression Linéaire \n",
    "\n",
    "La régression linéaire est l'une des méthodes les plus fondamentales et largement utilisées en apprentissage automatique et en statistiques. Elle sert à modéliser la relation entre une variable dépendante continue et une ou plusieurs variables indépendantes. Cette section vise à rafraîchir vos connaissances théoriques sur la régression linéaire avant de passer à l'implémentation pratique avec TensorFlow.\n",
    "\n",
    "## Qu'est-ce que la Régression Linéaire ?\n",
    "\n",
    "La régression linéaire cherche à **établir une relation linéaire** entre les variables indépendantes (également appelées **features** ou **prédicteurs**) et la variable dépendante (également appelée **cible** ou **réponse**). En d'autres termes, elle vise à **prédire** la valeur de la variable cible en fonction des valeurs des variables prédictrices.\n",
    "\n",
    "### Formulation Mathématique\n",
    "\n",
    "Pour une seule variable indépendante (régression linéaire simple), le modèle peut être exprimé comme :\n",
    "\n",
    "$\n",
    "y = w x + b\n",
    "$\n",
    "\n",
    "- $ y $ : Variable dépendante (cible)\n",
    "- $ x $ : Variable indépendante (feature)\n",
    "- $ w $ : Coefficient (poids) du modèle\n",
    "- $ b $ : Ordonnée à l'origine (biais)\n",
    "\n",
    "Dans le cas de plusieurs variables indépendantes (régression linéaire multiple), le modèle devient :\n",
    "\n",
    "$\n",
    "y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "$\n",
    "\n",
    "![Régression linéaire](linear.png)\n",
    "\n",
    "Ou, en notation vectorielle :\n",
    "\n",
    "$\n",
    "y = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$\n",
    "\n",
    "- $ \\mathbf{x} $ : Vecteur des variables indépendantes\n",
    "- $ \\mathbf{w} $ : Vecteur des poids associés aux variables indépendantes\n",
    "\n",
    "## Objectif de la Régression Linéaire\n",
    "\n",
    "L'objectif est de **trouver les valeurs optimales des poids $ \\mathbf{w} $ et du biais $ b $** qui minimisent la différence entre les valeurs prédites par le modèle et les valeurs réelles observées dans les données.\n",
    "\n",
    "### Fonction de Coût\n",
    "\n",
    "Pour quantifier cette différence, on utilise une **fonction de coût**. La fonction de coût la plus couramment utilisée pour la régression linéaire est **l'erreur quadratique moyenne** (Mean Squared Error, MSE) :\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} ( \\hat{y}^{(i)} - y^{(i)} )^2\n",
    "$$\n",
    "\n",
    "- $ m $ : Nombre d'exemples dans le jeu de données\n",
    "- $ y^{(i)} $ : Valeur réelle de la cible pour le $ i $-ème exemple\n",
    "- $ \\hat{y}^{(i)} = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b $ : Valeur prédite par le modèle pour le $ i $-ème exemple\n",
    "\n",
    "### Optimisation\n",
    "\n",
    "Pour minimiser la fonction de coût, on utilise des méthodes d'optimisation telles que **la descente de gradient**. L'idée est de mettre à jour les poids $ \\mathbf{w} $ et le biais $ b $ de manière itérative en se déplaçant dans la direction opposée au gradient de la fonction de coût.\n",
    "\n",
    "#### Mise à Jour des Poids et du Biais\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "w_j &:= w_j - \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial w_j} \\\\\n",
    "b &:= b - \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "- $ \\alpha $ : Taux d'apprentissage (learning rate)\n",
    "- $ \\frac{\\partial J}{\\partial w_j} $ : Dérivée partielle de la fonction de coût par rapport au poids $ w_j $\n",
    "- $ \\frac{\\partial J}{\\partial b} $ : Dérivée partielle de la fonction de coût par rapport au biais $ b $\n",
    "\n",
    "## Interprétation Géométrique\n",
    "\n",
    "- **Régression Linéaire Simple** : La relation entre $ x $ et $ y $ est représentée par une **droite** dans un plan 2D.\n",
    "- **Régression Linéaire Multiple** : La relation est représentée par un **hyperplan** dans un espace à n dimensions.\n",
    "\n",
    "## Hypothèses de la Régression Linéaire\n",
    "\n",
    "Pour que les estimations soient fiables, certaines hypothèses doivent être satisfaites :\n",
    "\n",
    "1. **Linéarité** : La relation entre les variables indépendantes et dépendante est linéaire.\n",
    "2. **Indépendance des Erreurs** : Les résidus (erreurs) sont indépendants les uns des autres.\n",
    "3. **Homoscedasticité** : La variance des erreurs est constante pour toutes les valeurs des variables indépendantes.\n",
    "4. **Normalité des Erreurs** : Les résidus sont distribués normalement.\n",
    "\n",
    "## Avantages et Limitations\n",
    "\n",
    "### Avantages\n",
    "\n",
    "- **Simplicité** : Facile à comprendre et à interpréter.\n",
    "- **Rapidité** : Peu coûteux en termes de calcul, adapté aux grands jeux de données.\n",
    "- **Interprétabilité** : Les coefficients indiquent l'impact de chaque variable indépendante sur la variable dépendante.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Linéarité** : Incapable de capturer des relations non linéaires.\n",
    "- **Sensibilité aux Outliers** : Les valeurs aberrantes peuvent fortement influencer le modèle.\n",
    "- **Multicolinéarité** : La corrélation entre les variables indépendantes peut affecter la stabilité des estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47709c01-9b33-4db3-aa7c-1b01f79d83f8",
   "metadata": {},
   "source": [
    "# Configuration de l'Environnement : Installation et Importation des Bibliothèques Nécessaires\n",
    "\n",
    "Avant de pouvoir implémenter notre modèle de régression linéaire avec TensorFlow, nous devons configurer notre environnement en installant et en important les bibliothèques nécessaires.\n",
    "\n",
    "## Étape 1 : Installation des Bibliothèques\n",
    "\n",
    "Pour ce tutoriel, nous aurons besoin de plusieurs bibliothèques Python :\n",
    "\n",
    "- **TensorFlow** : pour créer et entraîner notre modèle de régression linéaire.\n",
    "- **Pandas** : pour la manipulation des données.\n",
    "- **NumPy** : pour les calculs numériques.\n",
    "- **Matplotlib** : pour la visualisation des données.\n",
    "- **Scikit-Learn** : pour le prétraitement des données et le chargement de jeux de données.\n",
    "\n",
    "### Installation via pip\n",
    "\n",
    "Vous pouvez installer ces bibliothèques en utilisant la commande `pip`. Ouvrez votre terminal ou l'invite de commande et exécutez les commandes suivantes :\n",
    "\n",
    "```bash\n",
    "pip install tensorflow pandas numpy matplotlib scikit-learn\n",
    "```\n",
    "\n",
    "Assurez-vous que les installations se terminent sans erreur. Si vous utilisez un environnement virtuel (recommandé), assurez-vous d'activer cet environnement avant de procéder aux installations.\n",
    "\n",
    "## Étape 2 : Importation des Bibliothèques\n",
    "\n",
    "Une fois les bibliothèques installées, nous devons les importer dans notre notebook pour pouvoir les utiliser dans le code. Exécutez le code suivant dans une cellule Jupyter pour importer les bibliothèques :\n",
    "\n",
    "```python\n",
    "# Importation de TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importation pour la manipulation des données et les calculs numériques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importation pour la visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importation pour le prétraitement des données et le chargement de jeux de données\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "```\n",
    "\n",
    "## Explications des Bibliothèques Importées\n",
    "\n",
    "- **TensorFlow (`tensorflow`)** : Utilisé pour construire et entraîner notre modèle de régression linéaire.\n",
    "- **Pandas (`pandas`)** : Fournit des structures de données puissantes comme les DataFrames, facilitant la manipulation et l'analyse des données.\n",
    "- **NumPy (`numpy`)** : Une bibliothèque pour les calculs scientifiques avec des tableaux multidimensionnels, utile pour les opérations mathématiques.\n",
    "- **Matplotlib (`matplotlib.pyplot`)** : Permet de visualiser les données et les résultats du modèle.\n",
    "- **Scikit-Learn (`sklearn`)** : Une bibliothèque dédiée à l'apprentissage automatique, que nous utiliserons pour :\n",
    "  - Charger un jeu de données réel (`fetch_california_housing`).\n",
    "  - Diviser les données en ensembles d’entraînement et de test (`train_test_split`).\n",
    "  - Normaliser les données (`StandardScaler`).\n",
    "\n",
    "## Vérification de l'Installation de TensorFlow\n",
    "\n",
    "Pour vérifier que TensorFlow est bien installé et identifier si l'installation utilise un GPU (utile pour des calculs plus rapides), exécutez le code suivant :\n",
    "\n",
    "```python\n",
    "print(\"Version de TensorFlow :\", tf.__version__)\n",
    "\n",
    "# Vérifie si un GPU est disponible pour TensorFlow\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU disponible pour TensorFlow.\")\n",
    "else:\n",
    "    print(\"GPU non disponible pour TensorFlow. Exécution sur CPU.\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71927b47-bf82-426e-8c35-7d50f3f187ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bd6c1-7358-4093-b1b9-08e73c8248a7",
   "metadata": {},
   "source": [
    "# Chargement et Exploration des Données \n",
    "\n",
    "Maintenant que l'environnement est configuré, nous allons charger et explorer un jeu de données, nous utiliserons un ensemble de données sur les prix de l'immobilier en Californie, accessible via Scikit-Learn.\n",
    "\n",
    "1. **Chargement du Jeu de Données** : Utiliser `fetch_california_housing` de Scikit-Learn pour charger le jeu de données \"California Housing\" et afficher la description du jeu de données à l’aide de `print(housing.DESCR)`.\n",
    "\n",
    "2. **Conversion en DataFrame** : Convertir les données du jeu de données en un DataFrame Pandas pour une manipulation plus aisée. Ajouter la variable cible (`MedHouseVal`) dans le DataFrame.\n",
    "\n",
    "3. **Exploration Initiale des Données** :\n",
    "   - Afficher les premières lignes du DataFrame avec `print(df.head())`.\n",
    "   - Afficher des statistiques descriptives de base (moyenne, écart-type, min, max, etc.) avec `print(df.describe())`.\n",
    "   - Vérifier la présence de valeurs manquantes avec `print(df.isnull().sum())`.\n",
    "\n",
    "4. **Visualisation de la Variable Cible** : Créer un histogramme pour visualiser la distribution de la variable cible `MedHouseVal` (médiane des valeurs des maisons en Californie).\n",
    "\n",
    "5. **Visualisation des Corrélations** : Créer une matrice de corrélation avec un heatmap pour visualiser les relations entre les caractéristiques et la variable cible, afin d'identifier les caractéristiques les plus pertinentes pour la modélisation. \n",
    "\n",
    "Chaque étape correspond à une exploration ou une préparation des données avant de passer à la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceaf5b8b-b098-4f9f-9bf5-05b152d648ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b146b-084a-48ec-87fb-d0ca521e544a",
   "metadata": {},
   "source": [
    "# Prétraitement des Données : Normalisation et Division des Données en Ensembles d'Entraînement et de Test\n",
    "\n",
    "Avant de construire notre modèle de régression linéaire, nous devons préparer nos données. Le prétraitement est une étape cruciale qui permet d'améliorer les performances du modèle en optimisant la manière dont les données sont présentées au modèle.\n",
    "\n",
    "## Pourquoi Prétraiter les Données ?\n",
    "\n",
    "1. **Normalisation** : Étant donné que nos caractéristiques sont sur des échelles différentes (par exemple, la population dans un quartier peut être de plusieurs milliers tandis que l'âge médian des maisons est en dizaines), il est essentiel de normaliser les données pour éviter que des caractéristiques ayant de plus grandes valeurs dominent le modèle.\n",
    "2. **Division en Ensembles d'Entraînement et de Test** : Pour évaluer la performance du modèle de manière impartiale, nous séparons nos données en deux ensembles : un ensemble d’entraînement pour ajuster le modèle et un ensemble de test pour évaluer sa performance.\n",
    "## Étape 1 : Séparation des Caractéristiques et de la Cible\n",
    "\n",
    "Séparer les caractéristiques (features) de la variable cible (target) dans des variables différentes.\n",
    "\n",
    "- **`X`** : Contient toutes les caractéristiques (sauf la colonne `MedHouseVal`).\n",
    "- **`y`** : Contient la variable cible `MedHouseVal`.\n",
    "\n",
    "## Étape 2 : Division des Données en Ensembles d'Entraînement et de Test\n",
    "\n",
    "Diviser les données en deux parties : 80% pour l'entraînement et 20% pour le test. Cela permet de former le modèle sur la majorité des données tout en conservant une portion pour évaluer la performance du modèle.\n",
    "\n",
    "- **`X_train`, `y_train`** : Ensemble d'entraînement utilisé pour ajuster le modèle.\n",
    "- **`X_test`, `y_test`** : Ensemble de test utilisé pour évaluer la performance du modèle.\n",
    "\n",
    "## Étape 3 : Normalisation des Caractéristiques\n",
    "\n",
    "Normaliser les données pour garantir que toutes les caractéristiques sont sur une échelle similaire en utilisant le `StandardScaler` de Scikit-Learn, qui applique une transformation pour que chaque caractéristique ait une moyenne de 0 et un écart-type de 1.\n",
    "\n",
    "- **`fit_transform`** : Calculer les paramètres de normalisation (moyenne et écart-type) à partir des données d'entraînement, puis appliquer la transformation.\n",
    "- **`transform`** : Appliquer uniquement la transformation en utilisant les paramètres calculés à partir de l'ensemble d'entraînement.\n",
    "\n",
    "**Remarque** : Appliquer le `fit_transform` uniquement sur les données d'entraînement pour garantir que les données de test restent indépendantes des données d'entraînement.\n",
    "\n",
    "## Vérification de la Normalisation\n",
    "\n",
    "Vérifier que la normalisation a bien été appliquée en affichant les statistiques de base de `X_train` après transformation.\n",
    "\n",
    "La moyenne des caractéristiques normalisées devrait être proche de 0 et l'écart-type proche de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e52cc-f537-4e67-912c-4d4dbb4897d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015fbc9-741f-4642-ac5c-09c4c9ae9132",
   "metadata": {},
   "source": [
    "# Construction et Compilation du Modèle : Création d'un Modèle de Régression Linéaire avec TensorFlow\n",
    "\n",
    "Maintenant que nos données sont prêtes, nous allons construire et compiler notre modèle de régression linéaire en utilisant TensorFlow. Cette étape consiste à définir l'architecture de notre modèle et à choisir un optimiseur, une fonction de coût et des métriques pour l'entraînement.\n",
    "\n",
    "## Comprendre l'Architecture de Base du Modèle\n",
    "\n",
    "Pour une régression linéaire, notre modèle se compose d'une simple couche linéaire (ou couche dense) sans activation. Cette couche prendra les caractéristiques d'entrée (features) et produira une prédiction continue, qui est une estimation de notre variable cible (prix des maisons).\n",
    "\n",
    "### Structure du Modèle\n",
    "\n",
    "- **Entrée** : Notre modèle prendra un vecteur de caractéristiques (par exemple, surface des logements, population, nombre de chambres).\n",
    "- **Couche de sortie** : Une seule couche dense avec une seule unité (neurone) pour fournir une sortie continue.\n",
    "\n",
    "## Étape 1 : Création du Modèle\n",
    "\n",
    "Nous allons utiliser l'API Keras de TensorFlow pour construire notre modèle de manière séquentielle.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Création du modèle\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(X_train.shape[1],)),  # Définir la forme d'entrée\n",
    "    tf.keras.layers.Dense(1)  # Couche de sortie linéaire avec une unité\n",
    "])\n",
    "```\n",
    "\n",
    "### Explication du Code\n",
    "\n",
    "- `tf.keras.Sequential` : Permet de créer un modèle en empilant les couches de manière séquentielle.\n",
    "- `tf.keras.Input(shape=(X_train.shape[1],))` : Spécifie la forme des entrées du modèle, correspondant au nombre de caractéristiques dans `X_train`.\n",
    "- `tf.keras.layers.Dense(1)` : Ajoute une couche dense avec une unité de sortie pour prédire une valeur continue.\n",
    "\n",
    "## Étape 2 : Compilation du Modèle\n",
    "\n",
    "La compilation est une étape importante qui configure le modèle pour l'entraînement. Nous devons spécifier :\n",
    "\n",
    "- **Optimiseur** : Nous utiliserons l'optimiseur `Adam`, souvent choisi pour sa rapidité de convergence et sa stabilité.\n",
    "- **Fonction de Coût** : Pour la régression linéaire, la perte MSE (Mean Squared Error, ou erreur quadratique moyenne) est couramment utilisée pour mesurer l'écart entre les prédictions et les valeurs réelles.\n",
    "- **Métrique d'Évaluation** : Nous utiliserons `MAE` (Mean Absolute Error, ou erreur absolue moyenne), qui est souvent plus interprétable que la MSE, car elle donne la moyenne des écarts absolus en unités de la variable cible.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "```\n",
    "\n",
    "### Explication du Code\n",
    "\n",
    "- `optimizer='adam'` : Adam est un algorithme d'optimisation efficace pour ce type de problème.\n",
    "- `loss='mse'` : MSE (Mean Squared Error) est utilisée pour calculer l'erreur quadratique moyenne entre les prédictions et les valeurs réelles, ce qui est adapté aux problèmes de régression.\n",
    "- `metrics=['mae']` : MAE (Mean Absolute Error) est ajoutée pour évaluer le modèle en donnant l'erreur moyenne absolue des prédictions.\n",
    "\n",
    "## Vérification de l'Architecture du Modèle\n",
    "\n",
    "Il est toujours utile de vérifier la structure du modèle pour s'assurer que tout est configuré correctement. La commande `model.summary()` affiche un résumé des couches, des dimensions de sortie, et du nombre total de paramètres.\n",
    "\n",
    "```python\n",
    "# Afficher le résumé du modèle\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## Résumé de cette Étape\n",
    "\n",
    "1. **Construction du Modèle** : Nous avons créé un modèle séquentiel avec une seule couche dense pour effectuer une prédiction de régression.\n",
    "2. **Compilation du Modèle** : Nous avons configuré le modèle avec un optimiseur `Adam`, une fonction de coût `MSE`, et une métrique `MAE` pour évaluer les performances pendant l'entraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "856556c0-895f-4ccf-9f4d-639696477432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93632088-fbe5-44ac-b865-ed3925a8c728",
   "metadata": {},
   "source": [
    "# Entraînement du Modèle\n",
    "\n",
    "Maintenant que notre modèle est construit et compilé, nous allons l’entraîner en utilisant les données d'entraînement. L’entraînement est le processus où le modèle ajuste ses poids pour minimiser l'erreur entre les prédictions et les valeurs réelles.\n",
    "\n",
    "## Étape 1 : Définir le Nombre d'Époques et la Taille de Batch\n",
    "\n",
    "- **Nombre d'itérations (epochs)** : Le nombre de fois que le modèle passe sur l’ensemble des données d'entraînement. Un nombre plus élevé d’époques permet au modèle d’apprendre davantage, mais peut aussi mener à un surapprentissage (overfitting) s’il est trop élevé.\n",
    "- **Taille de batch (batch size)** : Le nombre d'échantillons traités avant la mise à jour des poids du modèle. Des tailles de batch plus petites permettent des mises à jour plus fréquentes mais peuvent augmenter le temps de calcul.\n",
    "\n",
    "Pour commencer, nous allons définir 50 époques et une taille de batch de 32, ce qui est raisonnable pour un modèle simple de régression linéaire.\n",
    "\n",
    "## Étape 2 : Entraîner le Modèle\n",
    "\n",
    "Nous utilisons la méthode `fit` de Keras pour entraîner notre modèle. Nous allons également utiliser une fraction des données d'entraînement comme validation pour surveiller la performance du modèle et détecter d'éventuels signes de surapprentissage.\n",
    "\n",
    "```python\n",
    "# Entraînement du modèle\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50,           # Nombre d'itérations\n",
    "    batch_size=32,       # Taille de batch\n",
    "    validation_split=0.2 # Fraction des données d'entraînement pour la validation\n",
    ")\n",
    "```\n",
    "\n",
    "### Explication du Code\n",
    "\n",
    "- `X_train` et `y_train` : Les données d'entraînement (caractéristiques et cible).\n",
    "- `epochs=50` : Le modèle passera 50 fois sur l’ensemble des données d'entraînement.\n",
    "- `batch_size=32` : Après chaque groupe de 32 échantillons, les poids du modèle sont mis à jour.\n",
    "- `validation_split=0.2` : 20 % des données d'entraînement sont réservées pour la validation, ce qui nous permet de suivre la performance du modèle sur des données \"nouvelles\" pendant l’entraînement.\n",
    "\n",
    "## Étape 3 : Suivi de la Convergence\n",
    "\n",
    "L'objet `history` contient les informations d'apprentissage du modèle (valeurs de perte et métrique) pour chaque époque. Nous pouvons l'utiliser pour visualiser la convergence de notre modèle et surveiller les performances sur les ensembles d'entraînement et de validation.\n",
    "\n",
    "### Visualisation des Courbes de Perte\n",
    "\n",
    "Pour mieux comprendre comment le modèle apprend, traçons les courbes de perte (MSE) pour les ensembles d'entraînement et de validation. Cela nous aide à repérer tout signe de surapprentissage si la perte de validation commence à augmenter pendant que celle de l'entraînement diminue.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracé des courbes de perte\n",
    "plt.plot(history.history['loss'], label='Perte d\\'entraînement')\n",
    "plt.plot(history.history['val_loss'], label='Perte de validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte (MSE)')\n",
    "plt.legend()\n",
    "plt.title(\"Convergence de la Perte du Modèle\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Visualisation des Courbes de l'Erreur Absolue Moyenne (MAE)\n",
    "\n",
    "Nous pouvons également tracer les courbes de MAE pour voir comment l'erreur moyenne absolue évolue au fil des époques.\n",
    "\n",
    "```python\n",
    "# Tracé des courbes de MAE\n",
    "plt.plot(history.history['mae'], label='MAE d\\'entraînement')\n",
    "plt.plot(history.history['val_mae'], label='MAE de validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Erreur absolue moyenne (MAE)')\n",
    "plt.legend()\n",
    "plt.title(\"Évolution de l'Erreur Absolue Moyenne (MAE)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Ces graphiques nous donnent une indication sur la façon dont le modèle s’améliore (ou non) au fur et à mesure de l'entraînement. Un écart important entre les performances d'entraînement et de validation peut indiquer un surapprentissage.\n",
    "\n",
    "## Résumé de l'Étape d'Entraînement\n",
    "\n",
    "1. **Définition des paramètres** : Nous avons défini un nombre d'époques et une taille de batch pour contrôler la vitesse d'apprentissage du modèle.\n",
    "2. **Entraînement du modèle** : Nous avons utilisé `model.fit` pour ajuster les poids du modèle sur l'ensemble d'entraînement.\n",
    "3. **Suivi de la convergence** : Nous avons tracé les courbes de perte et de MAE pour évaluer la progression du modèle pendant l'entraînement.\n",
    "\n",
    "Dans la prochaine section, nous allons évaluer la performance du modèle sur l'ensemble de test pour vérifier sa capacité de généralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b283a5-320c-4a35-b663-ca3961951250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd6c9c-0b59-4447-84ce-46ef5b9325a0",
   "metadata": {},
   "source": [
    "# Évaluation du Modèle : Mesure de la Performance du Modèle sur les Données de Test\n",
    "\n",
    "Après avoir entraîné notre modèle, nous devons évaluer sa performance sur un ensemble de données indépendant, appelé **ensemble de test**. Cela nous permet de vérifier si le modèle a bien généralisé et peut faire des prédictions précises sur des données qu'il n'a jamais vues.\n",
    "\n",
    "## Pourquoi Évaluer sur l’Ensemble de Test ?\n",
    "\n",
    "Pendant l’entraînement, le modèle apprend à minimiser l’erreur sur l’ensemble d’entraînement, mais il est important de vérifier que cette performance se traduit également sur des données nouvelles. Si le modèle fonctionne bien sur les données de test, cela indique qu'il n'a pas **surappris** (overfitting) et qu'il est capable de généraliser.\n",
    "\n",
    "## Étape 1 : Évaluation sur l’Ensemble de Test\n",
    "\n",
    "Nous utilisons la méthode `evaluate` pour calculer les valeurs de la perte (MSE) et de la métrique (MAE) sur l’ensemble de test. Cela nous donnera une idée claire de l'erreur moyenne que nous pouvons attendre du modèle.\n",
    "\n",
    "```python\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Perte (MSE) sur l'ensemble de test : {test_loss}\")\n",
    "print(f\"Erreur absolue moyenne (MAE) sur l'ensemble de test : {test_mae}\")\n",
    "```\n",
    "\n",
    "### Explication des Résultats\n",
    "\n",
    "- **MSE (Mean Squared Error)** : La perte MSE indique l'erreur quadratique moyenne, mesurant à quel point les prédictions s'éloignent des valeurs réelles en moyenne.\n",
    "- **MAE (Mean Absolute Error)** : La MAE est l'erreur moyenne absolue, qui indique directement la différence moyenne en unités de la variable cible entre les prédictions et les valeurs réelles. Elle est plus intuitive car elle est en unités de la variable cible.\n",
    "\n",
    "Des valeurs basses pour ces métriques indiquent que le modèle prédit les valeurs avec une bonne précision. Des valeurs plus élevées peuvent indiquer que le modèle a encore besoin d’améliorations.\n",
    "\n",
    "## Étape 2 : Visualisation des Prédictions vs Valeurs Réelles\n",
    "\n",
    "Pour mieux comprendre la performance du modèle, visualisons les valeurs prédites par le modèle par rapport aux valeurs réelles sur l'ensemble de test. Cela nous permet de voir si le modèle suit bien la tendance des données.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Tracé des valeurs réelles vs prédictions\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valeurs Réelles')\n",
    "plt.ylabel('Prédictions')\n",
    "plt.title('Valeurs Réelles vs Prédictions')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Interprétation du Graphique\n",
    "\n",
    "- **Nuage de points** : Chaque point représente une observation, avec la valeur réelle sur l'axe x et la prédiction sur l'axe y.\n",
    "- **Ligne de référence** : La ligne diagonale représente les points où la prédiction est exactement égale à la valeur réelle. Plus les points sont proches de cette ligne, plus les prédictions sont précises.\n",
    "\n",
    "Si les points sont dispersés loin de la ligne, cela indique que le modèle a des difficultés à prédire précisément certaines valeurs.\n",
    "\n",
    "## Résumé de l'Évaluation\n",
    "\n",
    "1. **Calcul de la Perte et de la MAE sur le Test** : Nous avons mesuré la performance du modèle en termes de MSE et MAE pour évaluer la précision de ses prédictions.\n",
    "2. **Visualisation des Prédictions** : Nous avons tracé les prédictions du modèle par rapport aux valeurs réelles pour observer visuellement la qualité de ses prédictions.\n",
    "\n",
    "Dans la dernière section, nous conclurons en passant en revue les points clés de notre travail et en discutant de potentielles améliorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859afa6-70be-4c4c-90f1-f143acde3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59109d-d171-448b-bdc3-17b74b2225ad",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "\n",
    "Pour appliquer la régression linéaire, plusieurs jeux de données sont intégrés dans les bibliothèques **scikit-learn** et **TensorFlow**.\n",
    "\n",
    "1. **Diabetes** :\n",
    "   - Description : Utilisé pour prédire la progression de la maladie du diabète un an après une première observation.\n",
    "   - Utilisation : `from sklearn.datasets import load_diabetes`\n",
    "   - Variables : 10 caractéristiques médicales (IMC, âge, pression artérielle, etc.) avec une cible quantitative.\n",
    "\n",
    "2. **Auto MPG** :\n",
    "   - Description : Prédiction de la consommation de carburant des voitures.\n",
    "   - Utilisation : Disponible via `http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data`\n",
    "   - Variables : Caractéristiques des véhicules (nombre de cylindres, poids, etc.) et consommation.\n",
    "\n",
    "3. **Concrete Compressive Strength** :\n",
    "   - Description : Prédiction de la résistance à la compression du béton en fonction de sa composition.\n",
    "   - Utilisation : Disponible via `https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls`\n",
    "   - Variables : Propriétés des matériaux utilisés dans le béton (quantités de composants, âge du béton).\n",
    "\n",
    "4. **Wine Quality** :\n",
    "   - Description : Peut être utilisé pour la régression pour prédire la qualité du vin sur une échelle de 0 à 10.\n",
    "   - Utilisation : Disponible via `https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv`.\n",
    "   - Variables : Caractéristiques chimiques des vins (acidité, sucre, pH, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070882d2-d125-42ee-8674-2efd119cff5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

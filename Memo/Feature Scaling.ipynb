{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<h1 align=\"center\">Feature Scaling</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is an essential step in data preprocessing for machine learning. It involves transforming the features (variables) of your dataset to a similar scale, which helps improve the performance and convergence of many machine learning algorithms.\n",
    "\n",
    "### **Why Feature Scaling is Important?**\n",
    "1. **Improves Algorithm Convergence**: Algorithms like gradient descent converge faster when features are on a similar scale.\n",
    "2. **Enhances Model Performance**: Distance-based algorithms such as k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and clustering methods like K-Means rely heavily on distances. Large-scale differences can disproportionately influence the model.\n",
    "3. **Avoids Dominance**: Without scaling, features with larger ranges may dominate others, leading to biased predictions.\n",
    "\n",
    "### **Types of Feature Scaling Techniques**\n",
    "\n",
    "1. **Min-Max Scaling (Normalization)**:\n",
    "   - **Description**: Scales the data to a fixed range, usually [0, 1].\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "     \\]\n",
    "   - **Use Case**: Ideal when you want to preserve relationships within the data while standardizing different features to the same scale.\n",
    "\n",
    "   **Example in Python**:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "   scaler = MinMaxScaler()\n",
    "   scaled_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "2. **Standardization (Z-Score Normalization)**:\n",
    "   - **Description**: Transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     X_{\\text{scaled}} = \\frac{X - \\text{mean}(X)}{\\text{std}(X)}\n",
    "     \\]\n",
    "   - **Use Case**: Preferred when the distribution of your data is Gaussian or when you don’t know the distribution.\n",
    "\n",
    "   **Example in Python**:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   scaler = StandardScaler()\n",
    "   standardized_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "3. **Robust Scaling**:\n",
    "   - **Description**: Uses the median and the interquartile range (IQR) instead of the mean and standard deviation. It’s robust to outliers.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     X_{\\text{scaled}} = \\frac{X - \\text{median}(X)}{\\text{IQR}(X)}\n",
    "     \\]\n",
    "   - **Use Case**: Effective when your data contains outliers.\n",
    "\n",
    "   **Example in Python**:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "   scaler = RobustScaler()\n",
    "   robust_scaled_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "4. **MaxAbs Scaling**:\n",
    "   - **Description**: Scales each feature by its maximum absolute value, transforming the data within the range [-1, 1]. It preserves sparsity, making it useful for sparse data.\n",
    "   - **Formula**:\n",
    "     \\[\n",
    "     X_{\\text{scaled}} = \\frac{X}{|X_{\\text{max}}|}\n",
    "     \\]\n",
    "   - **Use Case**: Suitable for sparse datasets or when negative values need to be retained.\n",
    "\n",
    "   **Example in Python**:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "   scaler = MaxAbsScaler()\n",
    "   max_abs_scaled_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "5. **Log Transformation**:\n",
    "   - **Description**: Applies a logarithmic function to compress a wide range of values and reduce skewness.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     X_{\\text{scaled}} = \\log(X + 1)\n",
    "     \\]\n",
    "   - **Use Case**: Useful for highly skewed data distributions and to reduce the impact of outliers.\n",
    "\n",
    "6. **Quantile Transformation**:\n",
    "   - **Description**: Transforms features to follow a uniform or normal distribution based on their quantiles. This technique uses rank-based mapping.\n",
    "   - **Use Case**: Suitable when you want to achieve a normal-like distribution for machine learning algorithms sensitive to the shape of the data distribution.\n",
    "\n",
    "   **Example in Python**:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "   transformer = QuantileTransformer(output_distribution='normal')\n",
    "   quantile_scaled_data = transformer.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "### **Choosing the Right Scaling Technique**\n",
    "- **Min-Max Scaling**: When features have similar distributions and you need values between 0 and 1.\n",
    "- **Standardization**: When features follow a normal distribution or are expected to have different units/scales.\n",
    "- **Robust Scaling**: When your data contains significant outliers.\n",
    "- **MaxAbs Scaling**: For sparse data or when negative values are crucial.\n",
    "- **Log Transformation**: When dealing with highly skewed data.\n",
    "\n",
    "### **Key Takeaways**:\n",
    "- **Scaling is essential** for models sensitive to distances, like SVMs, k-NN, and neural networks.\n",
    "- Be cautious about applying feature scaling on the training set and then on the test set separately to avoid data leakage.\n",
    "- It’s crucial to understand the data distribution before choosing a feature scaling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides a variety of tools for feature scaling through its `preprocessing` module. These tools make it simple to standardize or normalize your data as part of a machine learning pipeline. Here’s a closer look at each feature scaling method using scikit-learn:\n",
    "\n",
    "### **1. Min-Max Scaling (`MinMaxScaler`)**\n",
    "\n",
    "The `MinMaxScaler` transforms features to a fixed range, typically [0, 1]. This scaler preserves the relationships between data points but standardizes the scale.\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Create an instance of MinMaxScaler with a range between 0 and 1 (default)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "### **2. Standardization (`StandardScaler`)**\n",
    "\n",
    "`StandardScaler` standardizes features by removing the mean and scaling to unit variance. This is useful for algorithms like logistic regression, neural networks, and SVM.\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset\n",
    "data = [[1.0, 2.0], [2.0, 5.0], [3.0, 1.0], [4.0, 3.0]]\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "print(standardized_data)\n",
    "```\n",
    "\n",
    "### **3. Robust Scaling (`RobustScaler`)**\n",
    "\n",
    "`RobustScaler` uses the median and IQR to scale features, making it more resistant to outliers. This is ideal when outliers are present in your data.\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Example dataset with outliers\n",
    "data = [[1.0, 2.0], [2.0, 5.0], [3.0, 1.0], [100.0, 200.0]]\n",
    "\n",
    "# Create an instance of RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "robust_scaled_data = scaler.fit_transform(data)\n",
    "print(robust_scaled_data)\n",
    "```\n",
    "\n",
    "### **4. Max-Abs Scaling (`MaxAbsScaler`)**\n",
    "\n",
    "The `MaxAbsScaler` scales data based on the maximum absolute value, without shifting or centering the data. This method works well with sparse data.\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Example dataset\n",
    "data = [[1.0, -1.0], [2.0, 0.0], [4.0, 10.0], [3.0, -2.0]]\n",
    "\n",
    "# Create an instance of MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "maxabs_scaled_data = scaler.fit_transform(data)\n",
    "print(maxabs_scaled_data)\n",
    "```\n",
    "\n",
    "### **5. Power Transformation (`PowerTransformer`)**\n",
    "\n",
    "Scikit-learn also provides power-based transformations like Yeo-Johnson and Box-Cox to stabilize variance and make data more Gaussian-like.\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Example dataset\n",
    "data = [[1, 2], [2, 3], [2, 2], [3, 4]]\n",
    "\n",
    "# Create an instance of PowerTransformer (use 'yeo-johnson' or 'box-cox')\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Fit and transform the data\n",
    "power_scaled_data = scaler.fit_transform(data)\n",
    "print(power_scaled_data)\n",
    "```\n",
    "\n",
    "### **6. Quantile Transformation (`QuantileTransformer`)**\n",
    "\n",
    "The `QuantileTransformer` maps data to a uniform or normal distribution, based on quantiles. It’s useful for making features more evenly distributed.\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Example dataset\n",
    "data = [[1.0, 2.0], [2.0, 5.0], [3.0, 1.0], [4.0, 3.0]]\n",
    "\n",
    "# Create an instance of QuantileTransformer with 'normal' distribution\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "quantile_scaled_data = scaler.fit_transform(data)\n",
    "print(quantile_scaled_data)\n",
    "```\n",
    "\n",
    "### **Best Practices for Feature Scaling in Scikit-learn**:\n",
    "1. **Pipeline Integration**: It's common practice to use feature scaling within a `Pipeline` in scikit-learn to ensure consistency and prevent data leakage. This is especially crucial when scaling should only be fitted on training data.\n",
    "\n",
    "   **Example**:\n",
    "   ```python\n",
    "   from sklearn.pipeline import Pipeline\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "   # Create a pipeline with StandardScaler and Logistic Regression\n",
    "   pipeline = Pipeline([\n",
    "       ('scaler', StandardScaler()),\n",
    "       ('model', LogisticRegression())\n",
    "   ])\n",
    "\n",
    "   # Fit the pipeline on training data\n",
    "   pipeline.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "2. **Data Splitting**: Always split your data into training and testing sets before applying scaling. Fit the scaler on the training data and then transform both training and test data separately. This prevents data leakage and ensures that your model generalizes well.\n",
    "\n",
    "3. **Selecting a Scaling Method**: Choose the scaling method based on your algorithm and the nature of your data. For instance:\n",
    "   - Use `StandardScaler` for algorithms expecting normally distributed data or features centered at zero.\n",
    "   - Use `MinMaxScaler` for neural networks or algorithms that perform better with data between [0, 1].\n",
    "   - Use `RobustScaler` if your dataset contains outliers.\n",
    "\n",
    "Feature scaling helps improve the performance of many machine learning algorithms and makes sure that models converge faster and produce accurate results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

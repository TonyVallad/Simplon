{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">Weights & Biases (WandB)</h1>**\n",
    "\n",
    "<p align=\"center\"><i>Complete Guide to ML Experiment Tracking & Visualization</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2021/10/1.png\" alt=\"WandB\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Weights & Biases (WandB)?\n",
    "\n",
    "**Weights & Biases** is a machine learning platform that provides tools for experiment tracking, dataset versioning, and model management. It helps data scientists and ML engineers track, compare, and reproduce their machine learning experiments.\n",
    "\n",
    "### Key Problems WandB Solves:\n",
    "\n",
    "- **Experiment Tracking**: Keep track of hyperparameters, metrics, and model performance across multiple runs\n",
    "- **Visualization**: Create interactive plots and dashboards to understand model behavior\n",
    "- **Collaboration**: Share results and insights with team members\n",
    "- **Reproducibility**: Ensure experiments can be reproduced and compared\n",
    "- **Model Management**: Version control for models and datasets\n",
    "- **Hyperparameter Optimization**: Systematic search for optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Features\n",
    "\n",
    "### 1. **Experiment Tracking**\n",
    "- Log hyperparameters, metrics, and model outputs\n",
    "- Track system metrics (GPU utilization, memory usage)\n",
    "- Monitor training progress in real-time\n",
    "\n",
    "### 2. **Visualization**\n",
    "- Interactive plots and charts\n",
    "- Custom dashboards\n",
    "- Image, audio, and text logging\n",
    "- Model performance comparison\n",
    "\n",
    "### 3. **Hyperparameter Optimization**\n",
    "- Sweeps for systematic hyperparameter search\n",
    "- Bayesian optimization\n",
    "- Early stopping strategies\n",
    "\n",
    "### 4. **Artifacts**\n",
    "- Dataset versioning\n",
    "- Model versioning\n",
    "- File storage and lineage tracking\n",
    "\n",
    "### 5. **Reports**\n",
    "- Share findings with interactive reports\n",
    "- Combine visualizations with markdown\n",
    "- Collaborative documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install WandB\n",
    "!pip install wandb\n",
    "\n",
    "# For specific integrations\n",
    "!pip install wandb[media]  # For media logging (images, audio, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to WandB (requires account at wandb.ai)\n",
    "import wandb\n",
    "\n",
    "# Login using API key\n",
    "wandb.login()\n",
    "\n",
    "# Or login with API key directly\n",
    "# wandb.login(key=\"your_api_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize a new run\n",
    "wandb.init(\n",
    "    project=\"my-awesome-project\",  # Project name\n",
    "    name=\"experiment-1\",           # Run name (optional)\n",
    "    tags=[\"baseline\", \"v1\"],       # Tags for organization\n",
    "    config={                       # Hyperparameters\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 100,\n",
    "        \"model_type\": \"CNN\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access config\n",
    "config = wandb.config\n",
    "print(f\"Learning rate: {config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training loop\n",
    "for epoch in range(config.epochs):\n",
    "    # Simulate training metrics\n",
    "    loss = random.uniform(0.1, 2.0) * np.exp(-epoch * 0.1)\n",
    "    accuracy = min(0.95, random.uniform(0.5, 1.0) * (1 - np.exp(-epoch * 0.1)))\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"learning_rate\": config.learning_rate * (0.95 ** epoch)  # Decay\n",
    "    })\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize run\n",
    "run = wandb.init(project=\"advanced-logging\")\n",
    "\n",
    "# Log images\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "ax.plot(x, y)\n",
    "ax.set_title(\"Sample Plot\")\n",
    "\n",
    "wandb.log({\"sample_plot\": wandb.Image(fig)})\n",
    "plt.close()\n",
    "\n",
    "# Log tables\n",
    "data = [[i, i**2, i**3] for i in range(10)]\n",
    "table = wandb.Table(data=data, columns=[\"x\", \"x^2\", \"x^3\"])\n",
    "wandb.log({\"sample_table\": table})\n",
    "\n",
    "# Log histograms\n",
    "wandb.log({\"histogram\": wandb.Histogram(np.random.randn(1000))})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Popular Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Integration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"pytorch-integration\")\n",
    "\n",
    "# Define model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Watch the model (logs gradients and parameters)\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # Dummy data\n",
    "    x = torch.randn(32, 10)\n",
    "    y = torch.randn(32, 1)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow/Keras Integration\n",
    "import tensorflow as tf\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"tensorflow-integration\")\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Dummy data\n",
    "X_train = np.random.randn(1000, 10)\n",
    "y_train = np.random.randint(0, 2, (1000, 1))\n",
    "\n",
    "# Train with WandB callback\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[WandbCallback()]  # Automatically logs metrics\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Integration\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"sklearn-integration\")\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Log results\n",
    "wandb.log({\n",
    "    \"accuracy\": accuracy,\n",
    "    \"n_estimators\": 100,\n",
    "    \"feature_importance\": wandb.Histogram(rf.feature_importances_)\n",
    "})\n",
    "\n",
    "# Log classification report as table\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "wandb.log({\"classification_report\": report})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # or 'grid', 'random'\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-1\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd', 'rmsprop']\n",
    "        },\n",
    "        'hidden_units': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 32,\n",
    "            'max': 512\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"hyperparameter-sweep\")\n",
    "print(f\"Sweep ID: {sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for sweep\n",
    "def train():\n",
    "    # Initialize run\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Simulate training with hyperparameters\n",
    "    for epoch in range(20):\n",
    "        # Simulate metrics based on hyperparameters\n",
    "        loss = random.uniform(0.1, 1.0) * np.exp(-epoch * config.learning_rate)\n",
    "        val_accuracy = min(0.95, random.uniform(0.7, 0.95) * (1 - np.exp(-epoch * 0.1)))\n",
    "        \n",
    "        # Add some hyperparameter-dependent behavior\n",
    "        if config.optimizer == 'adam':\n",
    "            val_accuracy *= 1.05  # Adam tends to work better\n",
    "        elif config.optimizer == 'sgd':\n",
    "            val_accuracy *= 0.95\n",
    "        \n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "    \n",
    "    run.finish()\n",
    "\n",
    "# Run sweep (in practice, run this in separate processes/machines)\n",
    "# wandb.agent(sweep_id, train, count=5)  # Run 5 experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts: Dataset & Model Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and logging datasets as artifacts\n",
    "run = wandb.init(project=\"artifacts-demo\")\n",
    "\n",
    "# Create a sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000),\n",
    "    'feature2': np.random.randn(1000),\n",
    "    'target': np.random.randint(0, 2, 1000)\n",
    "})\n",
    "\n",
    "# Save dataset locally\n",
    "data.to_csv('dataset.csv', index=False)\n",
    "\n",
    "# Create artifact\n",
    "dataset_artifact = wandb.Artifact(\n",
    "    name=\"my-dataset\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Sample dataset for ML experiments\",\n",
    "    metadata={\"rows\": len(data), \"columns\": len(data.columns)}\n",
    ")\n",
    "\n",
    "# Add file to artifact\n",
    "dataset_artifact.add_file('dataset.csv')\n",
    "\n",
    "# Log artifact\n",
    "run.log_artifact(dataset_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using artifacts in another run\n",
    "run = wandb.init(project=\"artifacts-demo\")\n",
    "\n",
    "# Download and use artifact\n",
    "dataset_artifact = run.use_artifact('my-dataset:latest')\n",
    "dataset_dir = dataset_artifact.download()\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(f'{dataset_dir}/dataset.csv')\n",
    "print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "# Train a model and save it as artifact\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "X = data[['feature1', 'feature2']]\n",
    "y = data['target']\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "# Create model artifact\n",
    "model_artifact = wandb.Artifact(\n",
    "    name=\"my-model\",\n",
    "    type=\"model\",\n",
    "    description=\"Trained RandomForest model\"\n",
    ")\n",
    "model_artifact.add_file('model.pkl')\n",
    "run.log_artifact(model_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Project Organization**\n",
    "\n",
    "```python\n",
    "# Use descriptive project names\n",
    "wandb.init(project=\"image-classification-resnet\")\n",
    "\n",
    "# Use meaningful run names\n",
    "wandb.init(\n",
    "    project=\"my-project\",\n",
    "    name=f\"resnet50-lr{lr}-bs{batch_size}-{timestamp}\"\n",
    ")\n",
    "\n",
    "# Use tags for filtering\n",
    "wandb.init(\n",
    "    project=\"my-project\",\n",
    "    tags=[\"baseline\", \"resnet\", \"augmented\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. **Configuration Management**\n",
    "\n",
    "```python\n",
    "# Define all hyperparameters in config\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"model_architecture\": \"resnet50\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"data_augmentation\": True\n",
    "}\n",
    "\n",
    "wandb.init(project=\"my-project\", config=config)\n",
    "```\n",
    "\n",
    "### 3. **Consistent Logging**\n",
    "\n",
    "```python\n",
    "# Log at consistent intervals\n",
    "if step % log_interval == 0:\n",
    "    wandb.log({\n",
    "        \"step\": step,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_acc\n",
    "    })\n",
    "\n",
    "# Use meaningful metric names\n",
    "wandb.log({\n",
    "    \"train/loss\": train_loss,\n",
    "    \"train/accuracy\": train_acc,\n",
    "    \"val/loss\": val_loss,\n",
    "    \"val/accuracy\": val_acc\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Error Handling & Resource Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper error handling\n",
    "try:\n",
    "    run = wandb.init(project=\"my-project\")\n",
    "    \n",
    "    # Training code here\n",
    "    for epoch in range(100):\n",
    "        # Training logic\n",
    "        loss = train_epoch()\n",
    "        wandb.log({\"loss\": loss})\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    wandb.log({\"error\": str(e)})\n",
    "finally:\n",
    "    # Always finish the run\n",
    "    wandb.finish()\n",
    "\n",
    "# Or use context manager\n",
    "with wandb.init(project=\"my-project\") as run:\n",
    "    # Training code here\n",
    "    for epoch in range(100):\n",
    "        loss = train_epoch()\n",
    "        wandb.log({\"loss\": loss})\n",
    "    # wandb.finish() is called automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Performance Optimization**\n",
    "\n",
    "```python\n",
    "# Batch logging for efficiency\n",
    "metrics_buffer = []\n",
    "for step in range(1000):\n",
    "    # Training step\n",
    "    loss = compute_loss()\n",
    "    \n",
    "    metrics_buffer.append({\"step\": step, \"loss\": loss})\n",
    "    \n",
    "    # Log every N steps\n",
    "    if step % 10 == 0:\n",
    "        for metrics in metrics_buffer:\n",
    "            wandb.log(metrics)\n",
    "        metrics_buffer = []\n",
    "\n",
    "# Control logging frequency\n",
    "wandb.init(\n",
    "    project=\"my-project\",\n",
    "    settings=wandb.Settings(\n",
    "        _stats_sample_rate_seconds=60,  # System stats every 60s\n",
    "        _stats_samples_to_average=10    # Average over 10 samples\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics and plots\n",
    "run = wandb.init(project=\"advanced-features\")\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Generate data and train model\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Log ROC curve\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(y_test, y_proba)})\n",
    "\n",
    "# Log confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "    y_true=y_test, \n",
    "    preds=y_pred,\n",
    "    class_names=[\"Class 0\", \"Class 1\"]\n",
    ")})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model monitoring and alerts\n",
    "run = wandb.init(project=\"model-monitoring\")\n",
    "\n",
    "# Define alerts\n",
    "wandb.alert(\n",
    "    title=\"High Loss Alert\",\n",
    "    text=\"Training loss exceeded threshold\",\n",
    "    level=wandb.AlertLevel.WARN\n",
    ")\n",
    "\n",
    "# Custom summary metrics\n",
    "run.summary[\"best_accuracy\"] = 0.95\n",
    "run.summary[\"total_training_time\"] = \"2h 30m\"\n",
    "run.summary[\"final_model_size\"] = \"45MB\"\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Sharing Results**\n",
    "\n",
    "```python\n",
    "# Make runs public\n",
    "wandb.init(\n",
    "    project=\"my-project\",\n",
    "    settings=wandb.Settings(anonymous=\"allow\")\n",
    ")\n",
    "\n",
    "# Add notes to runs\n",
    "wandb.init(project=\"my-project\", notes=\"Baseline experiment with ResNet50\")\n",
    "\n",
    "# Create reports programmatically\n",
    "report = wandb.Api().create_report(\n",
    "    project=\"my-project\",\n",
    "    title=\"Weekly Model Performance\",\n",
    "    description=\"Summary of this week's experiments\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. **API Usage**\n",
    "\n",
    "```python\n",
    "# Query runs programmatically\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs from a project\n",
    "runs = api.runs(\"username/project-name\")\n",
    "\n",
    "# Filter runs\n",
    "runs = api.runs(\n",
    "    \"username/project-name\",\n",
    "    filters={\"config.learning_rate\": 0.001}\n",
    ")\n",
    "\n",
    "# Export data to DataFrame\n",
    "summary_list = []\n",
    "config_list = []\n",
    "for run in runs:\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "    config_list.append(run.config)\n",
    "\n",
    "runs_df = pd.DataFrame({\n",
    "    \"summary\": summary_list,\n",
    "    \"config\": config_list\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Use Cases\n",
    "\n",
    "### 1. **Deep Learning Experiments**\n",
    "- Track training loss, validation accuracy, learning curves\n",
    "- Log model architecture and hyperparameters\n",
    "- Monitor GPU utilization and training time\n",
    "- Version control datasets and trained models\n",
    "\n",
    "### 2. **Hyperparameter Optimization**\n",
    "- Systematic search across parameter spaces\n",
    "- Early stopping based on validation metrics\n",
    "- Parallel execution across multiple machines\n",
    "- Visualization of parameter importance\n",
    "\n",
    "### 3. **Model Comparison**\n",
    "- Compare different architectures side-by-side\n",
    "- A/B testing of model variants\n",
    "- Performance across different datasets\n",
    "- Reproducibility and result sharing\n",
    "\n",
    "### 4. **Production Monitoring**\n",
    "- Model performance drift detection\n",
    "- Data quality monitoring\n",
    "- Real-time inference metrics\n",
    "- Alert systems for anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB vs Alternatives\n",
    "\n",
    "| Feature | WandB | MLflow | TensorBoard | Neptune |\n",
    "|---------|-------|--------|-------------|----------|\n",
    "| **Ease of Use** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **Visualization** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **Collaboration** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **Cloud/Hosted** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Self-Hosted** | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ |\n",
    "| **Hyperparameter Optimization** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐ |\n",
    "| **Model Registry** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐ |\n",
    "| **Free Tier** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting & Tips\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Authentication Problems**\n",
    "   ```bash\n",
    "   # Re-login\n",
    "   wandb login --relogin\n",
    "   \n",
    "   # Check login status\n",
    "   wandb status\n",
    "   ```\n",
    "\n",
    "2. **Slow Logging**\n",
    "   ```python\n",
    "   # Reduce logging frequency\n",
    "   if step % 100 == 0:  # Log every 100 steps instead of every step\n",
    "       wandb.log(metrics)\n",
    "   ```\n",
    "\n",
    "3. **Large File Uploads**\n",
    "   ```python\n",
    "   # For large artifacts, use references instead\n",
    "   artifact.add_reference(\"s3://bucket/large-file.pkl\")\n",
    "   ```\n",
    "\n",
    "4. **Offline Mode**\n",
    "   ```python\n",
    "   # Work offline and sync later\n",
    "   wandb.init(mode=\"offline\")\n",
    "   \n",
    "   # Sync offline runs\n",
    "   # wandb sync path/to/offline/run\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "WandB is a powerful platform that significantly enhances the machine learning workflow by providing:\n",
    "\n",
    "- **Comprehensive Experiment Tracking**: Keep track of all experiments with detailed logging\n",
    "- **Beautiful Visualizations**: Interactive charts and dashboards for better insights\n",
    "- **Easy Collaboration**: Share results and collaborate with team members\n",
    "- **Reproducibility**: Ensure experiments can be reproduced and compared\n",
    "- **Integration**: Works seamlessly with popular ML frameworks\n",
    "- **Hyperparameter Optimization**: Systematic search for optimal parameters\n",
    "- **Model Management**: Version control for models and datasets\n",
    "\n",
    "### Getting Started:\n",
    "1. Create an account at [wandb.ai](https://wandb.ai)\n",
    "2. Install wandb: `pip install wandb`\n",
    "3. Login: `wandb login`\n",
    "4. Start logging your experiments!\n",
    "\n",
    "### Resources:\n",
    "- [Official Documentation](https://docs.wandb.ai/)\n",
    "- [Examples Gallery](https://wandb.ai/gallery)\n",
    "- [Community Forum](https://community.wandb.ai/)\n",
    "- [YouTube Tutorials](https://www.youtube.com/c/WeightsBiases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
